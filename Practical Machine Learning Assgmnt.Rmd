---
title: "Practical Machine Learning Assignment"
author: "Iffah Nabilah"
date: "February 14, 2016"
output: html_document
---
Executive Summary


One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal is to analyze data from accelerometers on the belt, forearm, arm, and dumbell of six participants. They were asked to perform barbell lifts correctly and incorrectly in five different ways. For more information see the "Weight Lifting Exercises Dataset" in the following location:

http://groupware.les.inf.puc-rio.br/har

Specifically, the goal of this machine learning exercise is to predict the manner in which the participants did the exercise-that is, to predict the "classe" variable found in the training set. The prediction model will then be used to predict twenty different test cases, as provided in the testing dataset.

Downloading the data:

=== Reading and Cleaning Data

We begin by loading the required libraries and reading in the training and testing datasets, assigning missing values to entries that are currently 'NA' or blank.


```{r}
library(caret)
set.seed(1234)

rawD <- read.csv("pml-training.csv", na.strings=c("NA",""), strip.white=T)

dim(rawD)

```

Applying the logical variable to the columns of the training and testing datasets will only keep those columns that are complete.

Our updated training dataset now has fewer variables to review in our analysis. Further, our final testing dataset has consistent columns in it (when compared with those in our slimmed-down training dataset). This will allow the fitted model (based on our training data) to be applied to the testing dataset.

```{r}

isNA <- apply(rawD, 2, function(x) { sum(is.na(x)) })
validData <- subset(rawD[, which(isNA == 0)], 
                    select=-c(X, user_name, new_window, num_window, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
dim(validData)

```

We now split the updated training dataset into a training dataset  and a validation dataset . This validation dataset will allow us to perform cross validation when developing our model.

```{r}

inTrain <- createDataPartition(validData$classe, p=0.7, list=F)
training <- validData[inTrain, ]
testing <- validData[-inTrain, ]

```

=== Training Random Forest Model

NOw train a Random Forest model on the training set and checking the prediction againts the held-back-test-set.  

```{r}
ctrl <- trainControl(allowParallel=T, method="cv", number=4)
model <- train(classe ~ ., data=training, model="rf", trControl=ctrl)
pred <- predict(model, newdata=testing)

sum(pred == testing$classe) / length(pred)
confusionMatrix(testing$classe, pred)$table
 
```                                    
                                 
Trained model is 99.4% accurate against our test-set and  confirmed by the confusion matrix. Let's use this super-accurate model to predict the unknown labels.   

```{r}

validTestData <- subset(rawD[, which(isNA == 0)], 
                        select=-c(X, user_name, new_window, num_window, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))
predict(model, newdata=validTestData)
        
```


Variables that are most important in this model?

```{r}
varImp(model)
```

=== Training a Simple Random Forest Model

Train and test a simpler model using only the top-ten most-important predictors.

```{r}

smallValidData <- subset(validData, 
                    select=c(roll_belt, pitch_forearm, yaw_belt, magnet_dumbbell_y, pitch_belt, magnet_dumbbell_z, roll_forearm, accel_dumbbell_y, roll_dumbbell, magnet_dumbbell_x,classe))
smallModel <- train(classe ~ ., data=smallValidData[inTrain,], model="rf", trControl=ctrl)

```

This is 5x faster and gets the same (correct) answer. Its accuracy on the test set is 98.5%.

```{r}
predict(smallModel, newdata=validTestData)
smallPred <- predict(smallModel, newdata=testing)
sum(smallPred == testing$classe) / length(smallPred)
confusionMatrix(testing$classe, smallPred)$table
```

